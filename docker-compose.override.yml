version: "3.1"

services:
  spark-master:
    image: bitnami/spark:latest
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_MASTER_WEBUI_PORT=8081
    ports:
      - "8082:8082"
      - "7077:7077"
    volumes:
      - spark-data:/bitnami
      - ./include:/usr/local/airflow/include
      - ./apps:/opt/spark-apps
      - ./data:/opt/spark-data
    networks:
      - airflow

  spark-worker:
    image: bitnami/spark:latest
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./include:/usr/local/airflow/include
      - spark-data:/bitnami
      - ./apps:/opt/spark-apps
      - ./data:/opt/spark-data
    depends_on:
      - spark-master
    networks:
      - airflow
  namenode:
    image: apache/hadoop:3.3.5
    container_name: namenode
    hostname: namenode
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
    volumes:
      - ./backend/hdfs/namenode/hadoop_namenode:/opt/hadoop/data/nameNode
      - ./backend/hdfs/hadoop_config:/opt/hadoop/etc/hadoop
      - ./backend/hdfs/namenode/start-hdfs.sh:/start-hdfs.sh
    ports:
      - "9870:9870"
    command: ["/bin/bash", "/start-hdfs.sh"]
    networks:
      - airflow

  datanode:
    image: apache/hadoop:3.3.5
    container_name: datanode
    hostname: datanode
    user: root
    environment:
      - HADOOP_HOME=/opt/hadoop
    volumes:
      - ./backend/hdfs/datanode/hadoop_datanode:/opt/hadoop/data/dataNode
      - ./backend/hdfs/hadoop_config:/opt/hadoop/etc/hadoop
      - ./backend/hdfs/datanode/init-datanode.sh:/init-datanode.sh
    depends_on:
      - namenode
    command: ["/bin/bash", "/init-datanode.sh"]
    networks:
      - airflow

  flask-app:
    build:
      context: ./backend/api/
    volumes:
      - ./backend/airflow/uploads:/app/uploads
    ports:
      - "5000:5000"
    environment:
      FLASK_APP: app.py
    depends_on:
      - namenode
    networks:
      - airflow

  frontend:
    build:
      context: ./front
    ports:
      - "3000:3000"
    environment:
      NODE_ENV: production
    volumes:
      - /app/.next
    networks:
      - airflow

volumes:
  spark-data:
